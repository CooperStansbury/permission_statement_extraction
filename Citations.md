# Citations

Here is a copy of the citations used for this project. 

[1]💫  Models for the spaCy Natural Language Processing (NLP) library: explosion/spacy-models. Explosion, 2019.
[2]C. Stansbury, A collection of textual analysis tools for documents: CooperStansbury/document_commandline_tools. 2019.
[3]“A domain analysis model for eIRB systems: Addressing the weak link in clinical research informatics - ScienceDirect.” [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1532046414001208. [Accessed: 14-Feb-2019].
[4]chm, “A guide to simple text classification with bert,” mc.ai, 21-Jan-2019. .
[5]Q. V. Le, “A Tutorial on Deep Learning Part 2: Autoencoders, Convolutional Neural Networks and Recurrent Neural Networks,” p. 20.
[6]A. Dumitrache, L. Aroyo, and C. Welty, “Achieving Expert-Level Annotation Quality with CrowdTruth,” p. 13.
[7]“ADAM_GuidanceDocument_15Dec2016_Final_v2.pdf,” Google Docs. [Online]. Available: https://drive.google.com/file/d/14pSiJFG9K6MiQClAhs5n2Y9ad0MhG7Hb/view?usp=sharing&usp=embed_facebook. [Accessed: 18-Feb-2019].
[8]“AI in healthcare: Big ethical questions still need answers,” Healthcare IT News, 13-Feb-2019. [Online]. Available: https://www.healthcareitnews.com/news/ai-healthcare-big-ethical-questions-still-need-answers. [Accessed: 14-Feb-2019].
[9]J.-J. C. Meyer, “Applications of Deontic Logic in Computer Science: A Concise Overview,” in Deontic Logic in Computer Science: Normative System Specification, 1993, pp. 17–40.
[10]R. F. AI, “Artificial Intelligence: A Modern Approach 3rd Edition PDF,” Ready For AI. .
[11]A. Vaswani et al., “Attention Is All You Need,” arXiv:1706.03762 [cs], Jun. 2017.
[12]“Automatable Discovery and Access Matrix,” Google Docs. [Online]. Available: https://drive.google.com/file/d/1E6gCZiu07WtKhQRLv87uw9rjj58RrAi9/view?usp=drive_open&usp=embed_facebook. [Accessed: 14-Feb-2019].
[13]J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” arXiv:1810.04805 [cs], Oct. 2018.
[14]H. L. Harrell and M. A. Rothstein, “Biobanking Research and Privacy Laws in the United States,” J Law Med Ethics, vol. 44, no. 1, pp. 106–127, Mar. 2016.
[15]“Biomedical Research Integrated Domain Group |.” [Online]. Available: https://bridgmodel.nci.nih.gov/. [Accessed: 20-Apr-2019].
[16]A. Grando and R. Schwab, “Building and evaluating an ontology-based tool for reasoning about consent permission,” AMIA Annu Symp Proc, vol. 2013, pp. 514–523, Nov. 2013.
[17]“can we use autoencoders for text data,” Stack Overflow. [Online]. Available: https://stackoverflow.com/questions/33794615/can-we-use-autoencoders-for-text-data. [Accessed: 08-Mar-2019].
[18]M. Dragoni, S. Villata, W. Rizzi, and G. Governatori, “Combining NLP Approaches for Rule Extraction from Legal Documents,” in 1st Workshop on MIning and REasoning with Legal texts (MIREL 2016), Sophia Antipolis, France, 2016.
[19]“Common Crawl.” .
[20]“Consent - FHIR v4.0.0.” [Online]. Available: https://www.hl7.org/fhir/consent.html. [Accessed: 20-Apr-2019].
[21]S. O. M. Dyke et al., “Consent Codes: Upholding Standard Data Use Conditions,” PLOS Genetics, vol. 12, no. 1, p. e1005772, Jan. 2016.
[22]Contribute to ga4gh/ADA-M development by creating an account on GitHub. Global Alliance for Genomics and Health, 2019.
[23]Deep Learning for humans. Contribute to keras-team/keras development by creating an account on GitHub. Keras, 2019.
[24]R. Socher, Y. Bengio, and C. D. Manning, “Deep Learning for NLP (Without Magic),” in Tutorial Abstracts of ACL 2012, Stroudsburg, PA, USA, 2012, pp. 5–5.
[25]P. McNamara, “Deontic Logic,” in The Stanford Encyclopedia of Philosophy, Fall 2018., E. N. Zalta, Ed. Metaphysics Research Lab, Stanford University, 2018.
[26]“Deontic logic,” Wikipedia. 09-Mar-2019.
[27]“Diagram of an artificial neural network,” TeX - LaTeX Stack Exchange. [Online]. Available: https://tex.stackexchange.com/questions/132444/diagram-of-an-artificial-neural-network. [Accessed: 16-Apr-2019].
[28]F. Ingham, “Dissecting BERT Part 2: BERT Specifics,” Dissecting BERT, 26-Nov-2018. .
[29]M. Mintz, S. Bills, R. Snow, and D. Jurafsky, “Distant supervision for relation extraction without labeled data,” in Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, Suntec, Singapore, 2009, pp. 1003–1011.
[30]A. J. J. Anglberger, “Dynamic Deontic Logic and Its Paradoxes,” Studia Logica: An International Journal for Symbolic Logic, vol. 89, no. 3, pp. 427–435, 2008.
[31]M.-T. Luong, H. Pham, and C. D. Manning, “Effective Approaches to Attention-based Neural Machine Translation,” Aug. 2015.
[32]C. Grady, “Enduring and Emerging Challenges of Informed Consent,” The New England Journal of Medicine; Boston, vol. 372, no. 9, pp. 855–862, Feb. 2015.
[33]K. Swampillai and M. Stevenson, “Extracting Relations Within and Across Sentences,” in Proceedings of the International Conference Recent Advances in Natural Language Processing 2011, Hissar, Bulgaria, 2011, pp. 25–32.
[34]“FAQ - Keras Documentation.” [Online]. Available: https://keras.io/getting-started/faq/. [Accessed: 20-Apr-2019].
[35]K. D. Mandl and I. S. Kohane, “Federalist principles for healthcare data networks,” Nature Biotechnology, vol. 33, no. 4, pp. 360–363, Apr. 2015.
[36]C. D. Manning and H. Schütze, Foundations of statistical natural language processing. Cambridge, Mass: MIT Press, 1999.
[37]“GA4GH.” .
[38]“GA4GH Connect A 5 year Strategic Plan,” p. 16.
[39]K. Lopyrev, “Generating News Headlines with Recurrent Neural Networks,” p. 9.
[40]A. See, P. J. Liu, and C. D. Manning, “Get To The Point: Summarization with Pointer-Generator Networks,” in Proceedings of the 55th Annual Meeting of the Association for           Computational Linguistics (Volume 1: Long Papers), Vancouver, Canada, 2017, pp. 1073–1083.
[41]J. Pennington, R. Socher, and C. Manning, “Glove: Global Vectors for Word Representation,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, 2014, pp. 1532–1543.
[42]“GloVe: Global Vectors for Word Representation.” [Online]. Available: https://nlp.stanford.edu/projects/glove/. [Accessed: 15-Apr-2019].
[43]J. Brownlee, “How to Develop a Character-Based Neural Language Model in Keras,” Machine Learning Mastery, 05-Nov-2017. .
[44]“I found great synonyms for ‘permission’ on the new Thesaurus.com!,” www.thesaurus.com. [Online]. Available: https://www.thesaurus.com/browse/permission. [Accessed: 19-Feb-2019].
[45]G. Bavota, A. De Lucia, and R. Oliveto, “Identifying Extract Class refactoring opportunities using structural and semantic cohesion measures,” Journal of Systems and Software, vol. 84, no. 3, pp. 397–414, Mar. 2011.
[46]Y. Kim, J. Geng, and H. Ney, “Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder,” in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018, pp. 862–868.
[47]“Informed Consent FAQs | HHS.gov.” [Online]. Available: https://www.hhs.gov/ohrp/regulations-and-policy/guidance/faq/informed-consent/index.html. [Accessed: 14-Feb-2019].
[48]Informed Consent Ontology. Contribute to ICO-Scrub-Team/ICO development by creating an account on GitHub. ICO-Scrub-Team, 2019.
[49]R. R. Faden, T. L. Beauchamp, and N. E. Kass, “Informed Consent, Comparative Effectiveness, and Learning Health Care,” New England Journal of Medicine, vol. 370, no. 8, pp. 766–768, Feb. 2014.
[50]Y. Chen and M. J. Zaki, “KATE: K-Competitive Autoencoder for Text,” arXiv:1705.02033 [cs, stat], May 2017.
[51]“Loss Functions — ML Cheatsheet documentation.” [Online]. Available: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy. [Accessed: 21-Apr-2019].
[52]T. M. Mitchell, Machine Learning. New York: McGraw-Hill, 1997.
[53]J. Landgrebe and B. Smith, “Making AI meaningful again,” arXiv:1901.02918 [cs], Jan. 2019.
[54]G. Kasneci, F. M. Suchanek, G. Ifrim, S. Elbassuoni, M. Ramanath, and G. Weikum, “NAGA: Harvesting, Searching and Ranking Knowledge,” in Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, New York, NY, USA, 2008, pp. 1285–1288.
[55]S. Bird, E. Klein, and E. Loper, Natural language processing with Python, 1st ed. Beijing ; Cambridge [Mass.]: O’Reilly, 2009.
[56]D. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate,” Sep. 2014.
[57]Z. Tu, Y. Liu, L. Shang, X. Liu, and H. Li, “Neural Machine Translation with Reconstruction,” in Thirty-First AAAI Conference on Artificial Intelligence, 2017.
[58]M. A. Nielsen, “Neural Networks and Deep Learning,” 2015.
[59]N. Yager, “Neural text generation,” Phrasee, 04-May-2018. .
[60]Y. Tsuruoka, J. McNaught, and S. Ananiadou, “Normalizing biomedical terms by minimizing ambiguity and variability,” BMC Bioinformatics, vol. 9, no. 3, p. S2, Apr. 2008.
[61]P. McNamara and H. Prakken, Norms, Logics and Information Systems: New Studies in Deontic Logic and Computer Science. IOS Press, 1999.
[62]N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, “On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima,” Sep. 2016.
[63]“Ontology.” [Online]. Available: https://www.cubrc.org/index.php/data-science-and-information-fusion/ontology. [Accessed: 20-Apr-2019].
[64]“OntoNotes Release 5.0 - Linguistic Data Consortium.” [Online]. Available: https://catalog.ldc.upenn.edu/LDC2013T19. [Accessed: 20-Apr-2019].
[65]“Penn Treebank II tag set | CLiPS,” 14-Oct-2010. [Online]. Available: http://www.clips.ua.ac.be/pages/mbsp-tags. [Accessed: 09-Apr-2019].
[66]O. for C. Rights (OCR), “Privacy,” HHS.gov, 07-May-2008. [Online]. Available: https://www.hhs.gov/hipaa/for-professionals/privacy/index.html. [Accessed: 20-Apr-2019].
[67]M. K. Paasche-Orlow, H. A. Taylor, and F. L. Brancati, “Readability Standards for Informed-Consent Forms as Compared with Actual Readability,” New England Journal of Medicine, vol. 348, no. 8, pp. 721–726, Feb. 2003.
[68]“Regulatory & Ethics Toolkit.” .
[69]C. Freitas, D. Santos, C. Mota, H. Gonçalo Oliveira, and P. Carvalho, “Relation detection between named entities: report of a shared task,” in Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), Boulder, Colorado, 2009, pp. 129–137.
[70]“Remove batchwise metrics · keras-team/keras@a56b1a5,” GitHub. [Online]. Available: https://github.com/keras-team/keras/commit/a56b1a55182acf061b1eb2e2c86b48193a0e88f7. [Accessed: 05-Apr-2019].
[71]“Revised Common Rule,” HHS.gov, 17-Jan-2017. [Online]. Available: https://www.hhs.gov/ohrp/regulations-and-policy/regulations/finalized-revisions-common-rule/index.html. [Accessed: 20-Apr-2019].
[72]J. Brownlee, “Save and Load Your Keras Deep Learning Models,” Machine Learning Mastery, 12-Jun-2016. .
[73]“scikit-learn: machine learning in Python — scikit-learn 0.20.2 documentation.” [Online]. Available: https://scikit-learn.org/stable/. [Accessed: 18-Feb-2019].
[74]Zhang Jiansong and El-Gohary Nora M., “Semantic NLP-Based Information Extraction from Construction Regulatory Documents for Automated Compliance Checking,” Journal of Computing in Civil Engineering, vol. 30, no. 2, p. 04015014, Mar. 2016.
[75]Y. Cheng et al., “Semi-Supervised Learning for Neural Machine Translation,” arXiv:1606.04596 [cs], Jun. 2016.
[76]J. Brownlee, “Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras,” Machine Learning Mastery, 25-Jul-2016. .
[77]N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, “SMOTE: Synthetic Minority Over-sampling Technique,” 1, vol. 16, pp. 321–357, Jun. 2002.
[78]“Software - The Stanford Natural Language Processing Group.” [Online]. Available: https://nlp.stanford.edu/software/. [Accessed: 21-Apr-2019].
[79]“spaCy · Industrial-strength Natural Language Processing in Python.” [Online]. Available: https://spacy.io/. [Accessed: 18-Feb-2019].
[80]D. Jurafsky and J. H. Martin, Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition, 2nd ed. Upper Saddle River, N.J: Pearson Prentice Hall, 2009.
[81]“StanfordNLP - Python NLP Library for Many Human Languages | StanfordNLP.” [Online]. Available: https://stanfordnlp.github.io/stanfordnlp/. [Accessed: 21-Apr-2019].
[82]“Statistics of Common Crawl Monthly Archives by commoncrawl.” [Online]. Available: https://commoncrawl.github.io/cc-crawl-statistics/. [Accessed: 20-Apr-2019].
[83]“Step 3: Prepare Your Data | ML Universal Guides,” Google Developers. [Online]. Available: https://developers.google.com/machine-learning/guides/text-classification/step-3. [Accessed: 21-Mar-2019].
[84]T. Kolda and B. Bader, “Tensor Decompositions and Applications,” SIAM Rev., vol. 51, no. 3, pp. 455–500, Aug. 2009.
[85]TensorFlow code and pre-trained models for BERT. Contribute to google-research/bert development by creating an account on GitHub. Google AI Research, 2019.
[86]E. R. Fonseca, Text autoencoder with LSTMs. Contribute to erickrf/autoencoder development by creating an account on GitHub. 2019.
[87]S. Ahamed, “Text Classification Using LSTM and visualize Word Embeddings: Part-1,” Medium, 09-Jan-2018. .
[88]text-autoencoder: Yoctol Natural Language Text Autoencoder. .
[89]The Common Core Ontology Repository holds the current released version of the Common Core Ontology suite. : CommonCoreOntology/CommonCoreOntologies. CommonCoreOntology, 2019.
[90]P. Johannesson and P. Wohed, “The deontic pattern – a framework for domain analysis in information systems design,” Data & Knowledge Engineering, vol. 31, no. 2, pp. 135–153, Sep. 1999.
[91]J. Alammar, “The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning).” [Online]. Available: http://jalammar.github.io/illustrated-bert/. [Accessed: 26-Mar-2019].
[92]J.-J. C. Meyer, R. J. Wieringa, and F. P. M. Dignum, “The Role of Deontic Logic in the Specification of Information Systems,” in Logics for Databases and Information Systems, J. Chomicki and G. Saake, Eds. Boston, MA: Springer US, 1998, pp. 71–115.
[93]H. I. Brown, “The Role of the Denominator in Bayes&#39; Theorem.”
[94]A. Halevy, P. Norvig, and F. Pereira, “The Unreasonable Effectiveness of Data,” IEEE Intelligent Systems, vol. 24, no. 2, pp. 8–12, Mar. 2009.
[95]B. Oshri and N. Khandwala, “There and Back Again: Autoencoders for Textual Reconstruction,” p. 9.
[96]M. B. Almeida, L. Slaughter, and M. Brochhausen, “Towards an Ontology of Document Acts: Introducing a Document Act Template for Healthcare,” in On the Move to Meaningful Internet Systems: OTM 2012 Workshops, 2012, pp. 420–425.
[97]“unbalanced classes - How do you apply SMOTE on text classification?,” Data Science Stack Exchange. [Online]. Available: https://datascience.stackexchange.com/questions/27671/how-do-you-apply-smote-on-text-classification. [Accessed: 02-Apr-2019].
[98]“Understanding binary cross-entropy / log loss: a visual explanation.” [Online]. Available: https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a. [Accessed: 21-Apr-2019].
[99]“WordNet | A Lexical Database for English.” [Online]. Available: https://wordnet.princeton.edu/. [Accessed: 20-Apr-2019].
[100]“WordNet Search - 3.1.” [Online]. Available: http://wordnetweb.princeton.edu/perl/webwn?s=permission&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&h=. [Accessed: 20-Apr-2019].
