{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERSION:  3.7.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(\"VERSION: \", python_version()) # expect 3.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
    "from spacy.tokens import Doc\n",
    "from spacy.vocab import Vocab\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random\n",
    "import itertools\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirty spaCy error workaround:\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the spaCy lib for vocab and vectors\n",
    "nlp = spacy.load('en')\n",
    "nlp_larg = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms_dir = '../input_data/all_forms'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Clues\n",
    "These are the verbs that we believe indicate the prescence of a statement of 'allowable action.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  taken from: \n",
    "# “I found synonyms for ‘permission’ on the new Thesaurus.com!,”\n",
    "# www.thesaurus.com. [Online]. Available: https://www.thesaurus.com/browse/permission. \n",
    "## [Accessed: 19-Feb-2019].\n",
    "\n",
    "permission_bases = [\"permission\", \n",
    "                    \"authorization\", \n",
    "                    \"authorize\",\n",
    "                    \"consent\",\n",
    "                    \"assent\",\n",
    "                    \"permit\",\n",
    "                    \"approve\",\n",
    "                    \"grant\",\n",
    "                    \"allow\",\n",
    "                    \"certify\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Synonyms\n",
    "Use WordNet to gather synonyms of the semantic clues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num elements in extended semantic clues: 50 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def addWordNetSynsets(word_list):\n",
    "    \" add synsets to new list \"\n",
    "    updated_list = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        updated_list.append(word)\n",
    "        synonyms = wordnet.synsets(word, 'v')\n",
    "        synonyms = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "        \n",
    "        [updated_list.append(x) for x in synonyms if x not in updated_list]\n",
    "        \n",
    "    return set(updated_list)\n",
    "        \n",
    "permission_extended = addWordNetSynsets(permission_bases)\n",
    "\n",
    "print('num elements in extended semantic clues:', len(permission_extended), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set_aside\n",
      "go_for\n",
      "assent\n",
      "permit\n",
      "give_up\n",
      "attest\n",
      "certify\n",
      "countenance\n",
      "accept\n",
      "approve\n",
      "acquiesce\n",
      "tolerate\n",
      "reserve\n",
      "authorization\n",
      "O.K.\n",
      "manifest\n",
      "deed_over\n",
      "pass\n",
      "indorse\n",
      "accord\n",
      "license\n",
      "appropriate\n",
      "grant\n",
      "empower\n",
      "accede\n",
      "licence\n",
      "permission\n",
      "leave\n",
      "earmark\n",
      "evidence\n",
      "sanction\n",
      "demonstrate\n",
      "allow_for\n",
      "let\n",
      "cede\n",
      "admit\n",
      "concede\n",
      "allow\n",
      "allot\n",
      "authorise\n",
      "authorize\n",
      "consent\n",
      "yield\n",
      "give\n",
      "provide\n",
      "endorse\n",
      "clear\n",
      "take_into_account\n",
      "okay\n",
      "award\n"
     ]
    }
   ],
   "source": [
    "for clue in permission_extended:\n",
    "    print(clue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Remove Inapproriate Clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "permission_extended = [\n",
    "    'accept',\n",
    "    'admit',\n",
    "    'permission',\n",
    "    'authorise',\n",
    "    'allow',\n",
    "    'give',\n",
    "    'sanction',\n",
    "    'assent',\n",
    "    'approve',\n",
    "    'give',\n",
    "    'authorization',\n",
    "    'accede',\n",
    "    'accord',\n",
    "    'permit',\n",
    "    'concede',\n",
    "    'attest',\n",
    "    'provide',\n",
    "    'grant',\n",
    "    'cede',\n",
    "    'authorize',\n",
    "    'let',\n",
    "    'allot',\n",
    "    'licence',\n",
    "    'certify',\n",
    "    'consent',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build consent form data structure\n",
    "get dataframe from each consent form on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(directory):\n",
    "    \"\"\" returns a dataframe with cleaned filenames, full paths,\n",
    "    and unprocessed text \"\"\"\n",
    "    \n",
    "    new_rows = []\n",
    "    \n",
    "    fileID = 0\n",
    "    \n",
    "    # iterate through directory\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            \n",
    "            fileID += 1\n",
    "            filepath = subdir + os.sep + file\n",
    "            \n",
    "            if filepath.endswith('.txt'):\n",
    "                wordList = [] # this will include duplicates and stop words\n",
    "                cleanedFileName = ''.join(e for e in file if e.isalnum())[:-3] \n",
    "                \n",
    "                # perform string operations on each file\n",
    "                with open(filepath, 'r') as myfile:\n",
    "                    data = myfile.read().replace('\\n', ' ')       \n",
    "                \n",
    "                new_rows.append(\n",
    "                    {\n",
    "                        'id': fileID,\n",
    "                        'name':cleanedFileName,\n",
    "                        'path':filepath,\n",
    "                        'rawText':data\n",
    "                    }\n",
    "                )\n",
    "    \n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>rawText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TAMUHRPPInformedconsent</td>\n",
       "      <td>../input_data/all_forms/TAMU - HRPP Informed c...</td>\n",
       "      <td>﻿NOT INTENDED FOR USE WITHOUT TAMU/BCD IRB APP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PotomacPrimaryCarefluconsentform</td>\n",
       "      <td>../input_data/all_forms/Potomac Primary Care_f...</td>\n",
       "      <td>PATIENT CONSENT FORM FOR SEASONAL INFLUENZA VA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OSUScheduledDeliveryConsent</td>\n",
       "      <td>../input_data/all_forms/OSU_Scheduled_Delivery...</td>\n",
       "      <td>SCHEDULED DELIVERY:           Today’s Date: Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>consentbiorepository121914</td>\n",
       "      <td>../input_data/all_forms/consent_biorepository_...</td>\n",
       "      <td>﻿ Informed Consent Form and HIPAA Authorizatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CambridgeConsentendodontics2</td>\n",
       "      <td>../input_data/all_forms/Cambridge_Consent_endo...</td>\n",
       "      <td>INFORMAT IONAL USE ONLY  CONSENT FOR ENDODONTI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                name  \\\n",
       "id                                     \n",
       "1            TAMUHRPPInformedconsent   \n",
       "2   PotomacPrimaryCarefluconsentform   \n",
       "3        OSUScheduledDeliveryConsent   \n",
       "4         consentbiorepository121914   \n",
       "5       CambridgeConsentendodontics2   \n",
       "\n",
       "                                                 path  \\\n",
       "id                                                      \n",
       "1   ../input_data/all_forms/TAMU - HRPP Informed c...   \n",
       "2   ../input_data/all_forms/Potomac Primary Care_f...   \n",
       "3   ../input_data/all_forms/OSU_Scheduled_Delivery...   \n",
       "4   ../input_data/all_forms/consent_biorepository_...   \n",
       "5   ../input_data/all_forms/Cambridge_Consent_endo...   \n",
       "\n",
       "                                              rawText  \n",
       "id                                                     \n",
       "1   ﻿NOT INTENDED FOR USE WITHOUT TAMU/BCD IRB APP...  \n",
       "2   PATIENT CONSENT FORM FOR SEASONAL INFLUENZA VA...  \n",
       "3   SCHEDULED DELIVERY:           Today’s Date: Da...  \n",
       "4   ﻿ Informed Consent Form and HIPAA Authorizatio...  \n",
       "5   INFORMAT IONAL USE ONLY  CONSENT FOR ENDODONTI...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE: running this cell will re-load the data in the dataframe from the dir.\n",
    "\"\"\"\n",
    "# run the function and store to variable \n",
    "df = getData(forms_dir)\n",
    "\n",
    "# set the index as the id, for future access\n",
    "df = df.set_index('id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sampling to speed up development\n",
    "This will be removed when we want to process the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out to run on whole document collection\n",
    "n_samples = 15\n",
    "df = df.sample(n=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Clean Text Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimalTextCleaning(row, field):\n",
    "    \"\"\" perform minimal text processing on raw data to new field \"\"\"\n",
    "    \n",
    "    cleaned_text = str(row[field]).lower() # lowercase\n",
    "    cleaned_text = re.sub(' +', ' ', cleaned_text) # strip redundant whitespace\n",
    "    return cleaned_text\n",
    "\n",
    "df['minimalCleaning'] = df.apply(lambda row:minimalTextCleaning(row, 'rawText'),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Raw Text to Spacy Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertFrom = 'rawText'\n",
    "convertTo = 'docOB'\n",
    "\n",
    "def getDocObjects(row, field):\n",
    "    \" return spacy doc object\"\n",
    "    doc = nlp_larg(str(row[field]).lower())\n",
    "    return doc\n",
    "\n",
    "df[convertTo] = df.apply(lambda row:getDocObjects(row, convertFrom),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertFrom = 'minimalCleaning'\n",
    "convertTo = 'cleaned_docOB'\n",
    "\n",
    "def getDocObjects(row, field):\n",
    "    \" return spacy doc object\"\n",
    "    doc = nlp_larg(str(row[field]).lower())\n",
    "    return doc\n",
    "\n",
    "df[convertTo] = df.apply(lambda row:getDocObjects(row, convertFrom),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>path</th>\n",
       "      <th>rawText</th>\n",
       "      <th>minimalCleaning</th>\n",
       "      <th>docOB</th>\n",
       "      <th>cleaned_docOB</th>\n",
       "      <th>sentList</th>\n",
       "      <th>candidatePermissionStatements</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>assenttemplate</td>\n",
       "      <td>../input_data/all_forms/assent-template.txt</td>\n",
       "      <td>﻿ COMIRB #:\\t Person in Charge of the Study: [...</td>\n",
       "      <td>﻿ comirb #:\\t person in charge of the study: [...</td>\n",
       "      <td>(﻿, comirb, #, :, \\t , person, in, charge, of,...</td>\n",
       "      <td>(﻿, comirb, #, :, \\t , person, in, charge, of,...</td>\n",
       "      <td>[(﻿, comirb, #, :, \\t , person, in, charge, of...</td>\n",
       "      <td>[(﻿, comirb, #, :, \\t , person, in, charge, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>BROOKLYNPERIODONTICSConsentforsurgicalconsent</td>\n",
       "      <td>../input_data/all_forms/BROOKLYNPERIODONTICS_C...</td>\n",
       "      <td>MICHAEL ZIDILE, D.D.S. P RAC T IC E LI MI T ED...</td>\n",
       "      <td>michael zidile, d.d.s. p rac t ic e li mi t ed...</td>\n",
       "      <td>(michael, zidile, ,, d.d.s, ., p, rac, t, ic, ...</td>\n",
       "      <td>(michael, zidile, ,, d.d.s, ., p, rac, t, ic, ...</td>\n",
       "      <td>[(michael, zidile, ,, d.d.s, .), (p, rac, t, i...</td>\n",
       "      <td>[(ti, st, r, y,  , consent, for, biopsy, with,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>CHOAICFHIPAATemplate</td>\n",
       "      <td>../input_data/all_forms/CHOA_ICFHIPAA_Template...</td>\n",
       "      <td>﻿  Emory University and Children’s Healthcare ...</td>\n",
       "      <td>﻿ emory university and children’s healthcare o...</td>\n",
       "      <td>(﻿,  , emory, university, and, children, ’s, h...</td>\n",
       "      <td>(﻿, emory, university, and, children, ’s, heal...</td>\n",
       "      <td>[(﻿,  , emory, university, and, children, ’s, ...</td>\n",
       "      <td>[(﻿,  , emory, university, and, children, ’s, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>Dukeexcesssampleconsentform</td>\n",
       "      <td>../input_data/all_forms/Duke-excess-sample-con...</td>\n",
       "      <td>Form M0345          DUKE UNIVERSITY HEALTH SYS...</td>\n",
       "      <td>form m0345 duke university health system conse...</td>\n",
       "      <td>(form, m0345,          , duke, university, hea...</td>\n",
       "      <td>(form, m0345, duke, university, health, system...</td>\n",
       "      <td>[(form, m0345,          , duke, university, he...</td>\n",
       "      <td>[(form, m0345,          , duke, university, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>FloridaInstituteofTechnologyConsentFormforF</td>\n",
       "      <td>../input_data/all_forms/Florida_Institute_of_T...</td>\n",
       "      <td>Consent Form TIV Inactivated Inﬂuenza Vaccine ...</td>\n",
       "      <td>consent form tiv inactivated inﬂuenza vaccine ...</td>\n",
       "      <td>(consent, form, tiv, inactivated, inﬂuenza, va...</td>\n",
       "      <td>(consent, form, tiv, inactivated, inﬂuenza, va...</td>\n",
       "      <td>[(consent, form, tiv, inactivated, inﬂuenza, v...</td>\n",
       "      <td>[(consent, form, tiv, inactivated, inﬂuenza, v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              name  \\\n",
       "id                                                   \n",
       "407                                 assenttemplate   \n",
       "135  BROOKLYNPERIODONTICSConsentforsurgicalconsent   \n",
       "154                           CHOAICFHIPAATemplate   \n",
       "640                    Dukeexcesssampleconsentform   \n",
       "205    FloridaInstituteofTechnologyConsentFormforF   \n",
       "\n",
       "                                                  path  \\\n",
       "id                                                       \n",
       "407        ../input_data/all_forms/assent-template.txt   \n",
       "135  ../input_data/all_forms/BROOKLYNPERIODONTICS_C...   \n",
       "154  ../input_data/all_forms/CHOA_ICFHIPAA_Template...   \n",
       "640  ../input_data/all_forms/Duke-excess-sample-con...   \n",
       "205  ../input_data/all_forms/Florida_Institute_of_T...   \n",
       "\n",
       "                                               rawText  \\\n",
       "id                                                       \n",
       "407  ﻿ COMIRB #:\\t Person in Charge of the Study: [...   \n",
       "135  MICHAEL ZIDILE, D.D.S. P RAC T IC E LI MI T ED...   \n",
       "154  ﻿  Emory University and Children’s Healthcare ...   \n",
       "640  Form M0345          DUKE UNIVERSITY HEALTH SYS...   \n",
       "205  Consent Form TIV Inactivated Inﬂuenza Vaccine ...   \n",
       "\n",
       "                                       minimalCleaning  \\\n",
       "id                                                       \n",
       "407  ﻿ comirb #:\\t person in charge of the study: [...   \n",
       "135  michael zidile, d.d.s. p rac t ic e li mi t ed...   \n",
       "154  ﻿ emory university and children’s healthcare o...   \n",
       "640  form m0345 duke university health system conse...   \n",
       "205  consent form tiv inactivated inﬂuenza vaccine ...   \n",
       "\n",
       "                                                 docOB  \\\n",
       "id                                                       \n",
       "407  (﻿, comirb, #, :, \\t , person, in, charge, of,...   \n",
       "135  (michael, zidile, ,, d.d.s, ., p, rac, t, ic, ...   \n",
       "154  (﻿,  , emory, university, and, children, ’s, h...   \n",
       "640  (form, m0345,          , duke, university, hea...   \n",
       "205  (consent, form, tiv, inactivated, inﬂuenza, va...   \n",
       "\n",
       "                                         cleaned_docOB  \\\n",
       "id                                                       \n",
       "407  (﻿, comirb, #, :, \\t , person, in, charge, of,...   \n",
       "135  (michael, zidile, ,, d.d.s, ., p, rac, t, ic, ...   \n",
       "154  (﻿, emory, university, and, children, ’s, heal...   \n",
       "640  (form, m0345, duke, university, health, system...   \n",
       "205  (consent, form, tiv, inactivated, inﬂuenza, va...   \n",
       "\n",
       "                                              sentList  \\\n",
       "id                                                       \n",
       "407  [(﻿, comirb, #, :, \\t , person, in, charge, of...   \n",
       "135  [(michael, zidile, ,, d.d.s, .), (p, rac, t, i...   \n",
       "154  [(﻿,  , emory, university, and, children, ’s, ...   \n",
       "640  [(form, m0345,          , duke, university, he...   \n",
       "205  [(consent, form, tiv, inactivated, inﬂuenza, v...   \n",
       "\n",
       "                         candidatePermissionStatements  \n",
       "id                                                      \n",
       "407  [(﻿, comirb, #, :, \\t , person, in, charge, of...  \n",
       "135  [(ti, st, r, y,  , consent, for, biopsy, with,...  \n",
       "154  [(﻿,  , emory, university, and, children, ’s, ...  \n",
       "640  [(form, m0345,          , duke, university, he...  \n",
       "205  [(consent, form, tiv, inactivated, inﬂuenza, v...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in df['docOB'].head(2):\n",
    "#     print((dir(i)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceList(row, field):\n",
    "    \"\"\" return list of sentences from doc object; each item will be token span \"\"\"\n",
    "    return list(row[field].sents)\n",
    "\n",
    "df['sentList'] = df.apply(lambda row:getSentenceList(row, 'docOB'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in df['sentList'].head(1):\n",
    "#     for a in i:\n",
    "#         print(a, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getWordListasString(row):\n",
    "#     \"\"\" return a list of words, including duplicates.\n",
    "#     NOTE: light cleaning on ingestion \"\"\"\n",
    "#     wordList = [] \n",
    "    \n",
    "#     for word in row['rawText'].split():\n",
    "#         word = re.sub(\"[^a-zA-Z]+\", \" \", word).strip().lower()\n",
    "#         if not word == \"\":\n",
    "#             wordList.append(word)\n",
    "            \n",
    "#     return \" \".join(wordList)\n",
    "        \n",
    "# df['cleanedText'] = df.apply(lambda row: getWordListasString(row),axis=1)\n",
    "\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preliminary permisison sentence extraction\n",
    "look for sentences that have a word from the 'clues' list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: remove duplicates, add count\n",
    "\n",
    "### NOTE: DO WE WANT SENTENCE PAIRS? THREE SENTENCES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "407    [(﻿, comirb, #, :, \\t , person, in, charge, of...\n",
       "135    [(ti, st, r, y,  , consent, for, biopsy, with,...\n",
       "Name: candidatePermissionStatements, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPossiblePermissions(row, permissions_list):\n",
    "    \"\"\" return list of sentences containing the \n",
    "    permissions words \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # iterate through a list of sentences\n",
    "    for sent in row['sentList']:\n",
    "        \n",
    "        # iterate through each clue\n",
    "        for clue in permissions_list:     \n",
    "            \n",
    "            # check if sting contains any clues (need to match case)\n",
    "            if sent.text.__contains__(clue):\n",
    "                \"\"\" NOTE: right not if ANY clue is found, this is enough \"\"\"\n",
    "                \n",
    "                candidates.append(sent)\n",
    "    \n",
    "    return candidates\n",
    "                \n",
    "# df.apply(lambda row:getPossiblePermissions(row, permission_extended),axis=1)\n",
    "\n",
    "df['candidatePermissionStatements'] = df.apply(lambda row:getPossiblePermissions(row, permission_extended),axis=1)\n",
    "\n",
    "df['candidatePermissionStatements'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent: ﻿ comirb #:\t person in charge of the study: [pi] version date:\t\t\t \t assent form for: [title]   what is this study about? \n",
      "\n",
      "sent: if i am in the study, i will: [one idea per bullet] \n",
      "\n",
      "sent: ti st r y  consent for biopsy with local anesthesia \n",
      "\n",
      "sent: understanding all of the above, i request that and hereby provide my informed consent to the treating doctor and his assistants to perform a biopsy. \n",
      "\n",
      "sent: ﻿  emory university and children’s healthcare of atlanta consent to be a research subject and hipaa authorization   \n",
      "\n",
      "sent: ﻿  emory university and children’s healthcare of atlanta consent to be a research subject and hipaa authorization   \n",
      "\n",
      "sent: form m0345          duke university health system          consent to participate in a research study         duhs biospecimen repository and processing core (brpc)          \n",
      "\n",
      "sent: facility pi: shannon mccall, md         participant category: excess tissue collection          this is a consent form for a research project. \n",
      "\n",
      "sent: consent form tiv inactivated inﬂuenza vaccine 2017  before consenting to receive the influenza vaccination, please answer the following questions. \n",
      "\n",
      "sent: the information you provide below is private and confidential and will not be used for any other purpose.   \n",
      "\n",
      "sent: patient consent form for seasonal influenza vaccine i have read, or have had explained to me, the cdc vaccine information statement about influenza and the influenza vaccine. \n",
      "\n",
      "sent: i understand the benefits and risks of influenza vaccine and request that the vaccine be given to me (or person named below for whom i am authorized to make this request). \n",
      "\n",
      "sent: ﻿irb #xxx-xxx\t\t\t\t\t\tconsent form version: xxx research participant information and consent form addendum university of wisconsin xxx department of xx – xx  study title:\t\t\txxx  sponsor:\t\t\txxx  principal investigator:\txx, m.d.    \n",
      "\n",
      "sent: when you enrolled in this research study, we told you we would let you know about any new information that might affect your willingness to take part in this study.   \n",
      "\n",
      "sent: kathy nordgren licensed acupuncturist  consent form for acupuncture   \n",
      "\n",
      "sent: i hereby authorize kathy nordgren, l.ac. \n",
      "\n",
      "sent: ﻿ the ohio state university assent to participate in research   study title:  researcher:  sponsor:     • you are being asked to be in a research study.   \n",
      "\n",
      "sent: if you decide you want to be in the study, an adult (usually a parent) will also need to give permission for you to be in the study.    \n",
      "\n",
      "sent: crestwood, mo 63126  phone: 314-984-8827 fax: 314-984-0736  mri procedure screening and consent form  patient name: sex: m date of birth:  date: f  physician: height:  weight:  attention: mri patients and accompanying family members \n",
      "\n",
      "sent: before you are allowed to enter, we must know if you have any metal in your body. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in df['candidatePermissionStatements'][:10]:\n",
    "    [print('sent:', x.text, '\\n') for x in sent[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getEstimatedPermissionDesnity(row):\n",
    "#     \"\"\" return proportion of sentence total that may\n",
    "#     be statements indicating perissions. Expect much noise. \"\"\"\n",
    "     \n",
    "#     return len(row['permissionsStatements'])/len(row['sentList'])\n",
    "    \n",
    "# df['permissionDensity'] = df.apply(lambda row: getEstimatedPermissionDesnity(row),axis=1)\n",
    "\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# df['permissionDensity'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "this represents an important break away from the primary dataframe. here i start to work with list structures to create a list of possible permissions that is no longer tied to filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'permissionsStatements'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2601\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2602\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2603\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'permissionsStatements'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d40fdca26468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mall_permissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpermlist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'permissionsStatements'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mall_permissions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpermlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2915\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2917\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2918\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2919\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2602\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'permissionsStatements'"
     ]
    }
   ],
   "source": [
    "all_sents = []\n",
    "\n",
    "for sentlist in df['sentList'].to_list():\n",
    "    [all_sents.append(x) for x in sentlist]\n",
    "    \n",
    "all_permissions = []\n",
    "\n",
    "for permlist in df['permissionsStatements'].to_list():\n",
    "    [all_permissions.append(x) for x in permlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total rough draft permissions: ', len(all_permissions))\n",
    "print('unique rough draft permissions: ', len(list(set(all_permissions))))\n",
    "\n",
    "print('total rough draft sentences: ', len(all_sents))\n",
    "print('unique rough draft sentences: ', len(list(set(all_sents))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplucates\n",
    "all_permissions = list(set(all_permissions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pair-wise similarity between our permission 'guesses' and remaining sentences\n",
    "this is an important step so that we don't restrict ourselves to permission statements that contain the words we dreampt up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING: WILL NOT SCALE, need to fix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rows = []\n",
    "\n",
    "for sent in all_sents:\n",
    "    for perm in all_permissions:\n",
    "        row = {\n",
    "            'permission':perm,\n",
    "            'sentence':sent,\n",
    "            'similarity':sent.similarity(perm)\n",
    "        }\n",
    "        new_rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = pd.DataFrame(new_rows)\n",
    "sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf['similarity'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dataframe, aggregate by mean sim score (weak sents will balance out)\n",
    "sf = sf.drop(columns=['permission'])\n",
    "sf = sf.groupby(['sentence'], as_index=False).mean()\n",
    "\n",
    "sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf['similarity'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sim = sf[sf.similarity > .7]\n",
    "candidates = high_sim['sentence'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total rough draft candidates: ', len(candidates))\n",
    "print('unique rough draft candidates: ', len(list(set(candidates))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print('sent: ', x, '\\n') for x in candidates[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_draft_permissions = all_permissions + candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total rough draft candidates: ', len(rough_draft_permissions))\n",
    "print('unique rough draft candidates: ', len(list(set(rough_draft_permissions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_draft_permissions = list(set(rough_draft_permissions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print statements to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import datetime\n",
    "# today = str(datetime.date.today())\n",
    "\n",
    "# file_path = 'statements-'+today+'.csv'\n",
    "# print(file_path)\n",
    "\n",
    "# with open(file_path, 'a') as outcsv:   \n",
    "#     #configure writer to write standard csv file\n",
    "#     writer = csv.writer(outcsv, delimiter=',', quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
    "#     writer.writerow(['raw_text', 'clean_text'])\n",
    "#     for perm in rough_draft_permissions:\n",
    "#         #Write item to outcsv\n",
    "#         raw_text = perm.text\n",
    "#         clean_text = re.sub('\\s+', ' ', re.sub('\\W+',' ',\\\n",
    "#                                re.sub('[^A-Za-z0-9]+',' ', \\\n",
    "#                                       re.sub(r'\\d+', \" \", raw_text)))).strip()\n",
    "#         writer.writerow([raw_text, clean_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotations?\n",
    "\n",
    "This is the point where injecting further information via annotations may be a good idea. This then would require in import of another local file and a small methods section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matrix representations\n",
    "\n",
    "a few different matrix representations of the permissions sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanPermissions(rough_draft_permissions):\n",
    "    \"string cleaning on permissions for td-idf ingestion\"\n",
    "    corpus = []\n",
    "    \n",
    "    \n",
    "    for permission in rough_draft_permissions:\n",
    "        cleanSentence = []\n",
    "        for word in permission.text.split():\n",
    "            word = re.sub(\"[^a-zA-Z]+\", \" \", word).strip().lower()\n",
    "            if not word == \"\":\n",
    "                cleanSentence.append(word)\n",
    "        corpus.append(\" \".join(cleanSentence))\n",
    "    return corpus\n",
    "            \n",
    "corpus = cleanPermissions(rough_draft_permissions)\n",
    "\n",
    "[print(x, '\\n') for x in corpus[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: \n",
    "`minTermFrequencyThreshold = 0` will result in all possible n_grams and will not scale as input size or ngramSize increases. However, it is the most robust representation of the sentence, and is worth exploring for the time being..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramSize = 5\n",
    "maxTermFrequencyThreshold = .8\n",
    "minTermFrequencyThreshold = .001\n",
    "\n",
    "def getTDIDFMatrix(corpus, ngram_range, max_df, min_df):\n",
    "    \"\"\" return td-idf matrix and terms \"\"\"\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, \n",
    "                                       ngram_range=(1,ngram_range),\n",
    "                                       max_df=max_df,min_df=min_df)\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    return tfidf_matrix, terms\n",
    "\n",
    "# save to a variable \n",
    "tdidf_matrix, tdidf_terms = getTDIDFMatrix(corpus, \n",
    "                                           ngramSize, \n",
    "                                           maxTermFrequencyThreshold,\n",
    "                                           minTermFrequencyThreshold)\n",
    "\n",
    "# # print tests\n",
    "# print('\\nfirst few terms:')\n",
    "# [print(\" \", x) for x in tdidf_terms[:10]]\n",
    "\n",
    "print('\\nNumber of terms:', len(tdidf_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.SparseDataFrame(tdidf_matrix, columns=tdidf_terms)\n",
    "\n",
    "# here we add the sentences back in\n",
    "sdf['sent'] = corpus\n",
    "\n",
    "sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permission_vectors = []\n",
    "\n",
    "for perm in rough_draft_permissions:\n",
    "#     print(perm.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])) # same features, different rows\n",
    "#     print(perm.vector) # same shape\n",
    "#     print(perm.vector_norm) # single value\n",
    "#     print(perm.get_lca_matrix()) # differnt shapes\n",
    "    permission_vectors.append(perm.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
